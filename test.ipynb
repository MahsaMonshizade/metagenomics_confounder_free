{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values\n",
      "METFORMIN_C\n",
      "1    373\n",
      "0    298\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dcor\n",
    "\n",
    "# Pearson Correlation Loss\n",
    "class PearsonCorrelationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PearsonCorrelationLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        x = target\n",
    "        y = pred\n",
    "        mx = torch.mean(x)\n",
    "        my = torch.mean(y)\n",
    "        xm = x - mx\n",
    "        ym = y - my\n",
    "        r_num = torch.sum(xm * ym)\n",
    "        r_den = torch.sqrt(torch.sum(xm ** 2) * torch.sum(ym ** 2)) + 1e-5\n",
    "        r = r_num / r_den\n",
    "        r = torch.clamp(r, min=-1.0, max=1.0)\n",
    "        return r ** 2\n",
    "\n",
    "def previous_power_of_two(x):\n",
    "    \"\"\"Return the largest power of two less than or equal to x.\"\"\"\n",
    "    return 1 << (x - 1).bit_length() - 1\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, latent_dim=64, num_layers=1):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Building the encoder\n",
    "        self.encoder = self._build_encoder(input_size, latent_dim, num_layers, nn.ReLU())\n",
    "        \n",
    "        # Building the classifier\n",
    "        self.classifier = self._build_classifier_adv(latent_dim, nn.Tanh(), num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward pass\n",
    "        encoded = self.encoder(x)\n",
    "        # Classifier forward pass\n",
    "        output = self.classifier(encoded)\n",
    "        return output, encoded\n",
    "\n",
    "    def _build_encoder(self, input_dim, latent_dim, num_layers, activation_fn):\n",
    "        \"\"\"Build the encoder network.\"\"\"\n",
    "        layers = []\n",
    "        first_layer = previous_power_of_two(input_dim)\n",
    "        layers.extend([\n",
    "            nn.Linear(input_dim, first_layer),\n",
    "            nn.BatchNorm1d(first_layer),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        current_dim = first_layer\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, current_dim // 2),\n",
    "                nn.BatchNorm1d(current_dim // 2),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_dim = current_dim // 2\n",
    "        layers.extend([\n",
    "            nn.Linear(current_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _build_classifier_adv(self, latent_dim, activation_fn, num_layers):\n",
    "        \"\"\"Build the disease classifier.\"\"\"\n",
    "        layers = []\n",
    "        current_dim = latent_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, current_dim // 2),\n",
    "                nn.BatchNorm1d(current_dim // 2),\n",
    "                nn.Tanh()\n",
    "            ])\n",
    "            current_dim = current_dim // 2\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "# # Simple Feedforward Neural Network\n",
    "# class SimpleModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(SimpleModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "#         self.sigmoid = nn.Sigmoid()  # Sigmoid to get probabilities between 0 and 1\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         hidden_activations = out.clone()  # Get activations after ReLU\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.sigmoid(out)  # Apply sigmoid activation to get probabilities\n",
    "#         return out, hidden_activations  # Return both output and hidden activations\n",
    "\n",
    "def load_and_transform_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from CSV, apply CLR transformation, and return transformed DataFrame with 'uid'.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    uid = data['SampleID']\n",
    "    X = data.drop(columns=['SampleID']).values\n",
    "    X_log = np.log(X + 1)\n",
    "    X_log_df = pd.DataFrame(X_log, columns=data.columns[1:])\n",
    "    X_log_df['SampleID'] = uid\n",
    "    return X_log_df[['SampleID'] + list(X_log_df.columns[:-1])]\n",
    "\n",
    "# Generate synthetic data with 654 features and imbalanced labels\n",
    "def generate_synthetic_data():\n",
    "    \n",
    "\n",
    "    file_path = 'MetaCardis_data/train_T2D_abundance.csv'\n",
    "    metadata_file_path = 'MetaCardis_data/train_T2D_metadata.csv'\n",
    "    relative_abundance = load_and_transform_data(file_path)\n",
    "    metadata = pd.read_csv(metadata_file_path)\n",
    "    print(\"values\")\n",
    "    print(metadata['METFORMIN_C'].value_counts())\n",
    "    features = relative_abundance.drop(columns=['SampleID']).values\n",
    "    x = torch.tensor(features, dtype=torch.float32)\n",
    "    y = torch.tensor(metadata['METFORMIN_C'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    return x , y\n",
    "\n",
    "# Training function with DataLoader\n",
    "def train_model(model, criterion, optimizer, data_loader, num_epochs=500):\n",
    "    loss_history = []\n",
    "    dcor_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        hidden_activations_list = []\n",
    "        targets_list = []\n",
    "        \n",
    "        for x_batch, y_batch in data_loader:\n",
    "            # Forward pass\n",
    "            outputs, hidden_activations = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Collect hidden activations and targets\n",
    "            hidden_activations_list.append(hidden_activations.detach().cpu())\n",
    "            targets_list.append(y_batch.detach().cpu())\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        # After epoch, concatenate hidden activations and targets\n",
    "        hidden_activations_all = torch.cat(hidden_activations_list, dim=0)\n",
    "        targets_all = torch.cat(targets_list, dim=0)\n",
    "    \n",
    "        # Compute distance correlation between hidden activations and targets\n",
    "        hidden_activations_np = hidden_activations_all.numpy()\n",
    "        targets_np = targets_all.numpy()\n",
    "        dcor_value = dcor.distance_correlation_sqr(hidden_activations_np, targets_np)\n",
    "        dcor_history.append(dcor_value)\n",
    "        \n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, DCor: {dcor_value:.4f}')\n",
    "    \n",
    "    return loss_history, dcor_history\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_size = 654  # 654 features\n",
    "    hidden_size = 32\n",
    "    output_size = 1\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 200\n",
    "    batch_size = 64\n",
    "\n",
    "    # Generate synthetic data with 654 features and imbalanced labels\n",
    "    x_train, y_train = generate_synthetic_data()\n",
    "\n",
    "    # Compute class counts and proportions\n",
    "    labels = y_train.squeeze()\n",
    "    unique_labels = labels.unique()\n",
    "    class_counts = {label.item(): (labels == label).sum().item() for label in unique_labels}\n",
    "    total_samples = len(labels)\n",
    "    class_proportions = {label: count / total_samples for label, count in class_counts.items()}\n",
    "\n",
    "    # Compute samples per class per batch\n",
    "    import math\n",
    "\n",
    "    samples_per_class = {}\n",
    "    remainders = {}\n",
    "    total_samples_in_batch = 0\n",
    "\n",
    "    for label, proportion in class_proportions.items():\n",
    "        exact_samples = proportion * batch_size\n",
    "        samples = int(math.floor(exact_samples))\n",
    "        remainder = exact_samples - samples\n",
    "        samples_per_class[label] = samples\n",
    "        remainders[label] = remainder\n",
    "        total_samples_in_batch += samples\n",
    "\n",
    "    # Distribute remaining slots based on the largest remainders\n",
    "    remaining_slots = batch_size - total_samples_in_batch\n",
    "    sorted_labels = sorted(remainders.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i in range(remaining_slots):\n",
    "        label = sorted_labels[i % len(sorted_labels)][0]\n",
    "        samples_per_class[label] += 1\n",
    "\n",
    "    # Get indices for each class and shuffle them\n",
    "    class_indices = {label.item(): (labels == label).nonzero(as_tuple=True)[0] for label in unique_labels}\n",
    "    for label in class_indices:\n",
    "        indices = class_indices[label]\n",
    "        class_indices[label] = indices[torch.randperm(len(indices))]\n",
    "\n",
    "    # Generate stratified batches\n",
    "    def stratified_batches(class_indices, samples_per_class, batch_size):\n",
    "        batches = []\n",
    "        class_cursors = {label: 0 for label in class_indices}\n",
    "        num_samples = sum([len(indices) for indices in class_indices.values()])\n",
    "        num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            batch = []\n",
    "            for label, indices in class_indices.items():\n",
    "                cursor = class_cursors[label]\n",
    "                samples = samples_per_class[label]\n",
    "                # If we've run out of samples for this class, skip\n",
    "                if cursor >= len(indices):\n",
    "                    continue\n",
    "                # Adjust samples if not enough samples left\n",
    "                if cursor + samples > len(indices):\n",
    "                    samples = len(indices) - cursor\n",
    "                batch_indices = indices[cursor:cursor+samples]\n",
    "                batch.extend(batch_indices.tolist())\n",
    "                class_cursors[label] += samples\n",
    "            # Shuffle batch indices\n",
    "            if batch:\n",
    "                batch = torch.tensor(batch)[torch.randperm(len(batch))].tolist()\n",
    "                batches.append(batch)\n",
    "        return batches\n",
    "\n",
    "    batches = stratified_batches(class_indices, samples_per_class, batch_size)\n",
    "\n",
    "    # Create a custom BatchSampler\n",
    "    class StratifiedBatchSampler(torch.utils.data.BatchSampler):\n",
    "        def __init__(self, batches):\n",
    "            self.batches = batches\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.batches:\n",
    "                yield batch\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.batches)\n",
    "\n",
    "    # Create a dataset and a DataLoader with the custom BatchSampler\n",
    "    dataset = TensorDataset(x_train, y_train)\n",
    "    batch_sampler = StratifiedBatchSampler(batches)\n",
    "    data_loader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "    # Define model, loss, and optimizer\n",
    "    model = SimpleModel(input_size, hidden_size, output_size)\n",
    "    criterion = PearsonCorrelationLoss()  # Use correlation loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    loss_history, dcor_history = train_model(model, criterion, optimizer, data_loader, num_epochs)\n",
    "\n",
    "    # Plot the loss history and distance correlation history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"Loss History\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(dcor_history)\n",
    "    plt.title(\"Distance Correlation History\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Distance Correlation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Testing the model with new data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_x = torch.randn(3, input_size)  # Test inputs with 654 features\n",
    "        predictions, _ = model(test_x)\n",
    "        print(\"Test Inputs: \", test_x)\n",
    "        print(\"Predictions (Probabilities): \", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confounder_free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
