# Yuhui: This code is generated by Claude Sonnet 4, 
#      please pay attention to the correctness and modify it if needed. 
# Comparing two models on the same dataset using DeLong's test
# e.g. prob1, label1, sampleid1: Results/FCNN_encoder_confounder_free_plots/test_results.npz
#      prob2, labe2, sampleid2: Results/FCNN_plots/test_results.npz

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from scipy import stats
import pandas as pd

def mcnemar_test(y_true, y_pred1, y_pred2):
    """
    Perform McNemar's test to compare two classifiers on the same dataset.
    
    Parameters:
    y_true: array-like, true labels
    y_pred1: array-like, predictions from model 1
    y_pred2: array-like, predictions from model 2
    
    Returns:
    dict: Contains test statistics, p-value, and contingency table
    """
    
    # Create contingency table
    # Format: 
    #         Model 2 Correct    Model 2 Wrong
    # Model 1 Correct    a            b
    # Model 1 Wrong      c            d
    
    correct1 = (y_pred1 == y_true)
    correct2 = (y_pred2 == y_true)
    
    a = np.sum(correct1 & correct2)      # Both correct
    b = np.sum(correct1 & ~correct2)     # Model 1 correct, Model 2 wrong
    c = np.sum(~correct1 & correct2)     # Model 1 wrong, Model 2 correct
    d = np.sum(~correct1 & ~correct2)    # Both wrong
    
    contingency_table = np.array([[a, b], [c, d]])
    
    # McNemar's test focuses on discordant pairs (b and c)
    if b + c == 0:
        # No discordant pairs - models perform identically
        chi2_stat = 0
        p_value = 1.0
    elif b + c < 25:
        # Use exact binomial test for small samples
        p_value = 2 * stats.binom.cdf(min(b, c), b + c, 0.5)
        chi2_stat = np.nan  # Not applicable for exact test
    else:
        # Use chi-square approximation with continuity correction
        chi2_stat = (abs(b - c) - 1)**2 / (b + c)
        p_value = 1 - stats.chi2.cdf(chi2_stat, df=1)
    
    return {
        'contingency_table': contingency_table,
        'discordant_pairs': b + c,
        'chi2_stat': chi2_stat,
        'p_value': p_value,
        'model1_better': b,
        'model2_better': c,
        'both_correct': a,
        'both_wrong': d
    }

def paired_accuracy_test(y_true, y_pred1, y_pred2):
    """
    Compare accuracy of two models using paired t-test approach.
    
    Parameters:
    y_true: array-like, true labels
    y_pred1: array-like, predictions from model 1
    y_pred2: array-like, predictions from model 2
    
    Returns:
    dict: Contains accuracies, difference, and statistical test results
    """
    
    # Calculate per-sample correctness
    correct1 = (y_pred1 == y_true).astype(int)
    correct2 = (y_pred2 == y_true).astype(int)
    
    # Calculate overall accuracies
    acc1 = np.mean(correct1)
    acc2 = np.mean(correct2)
    acc_diff = acc1 - acc2
    
    # Paired t-test on per-sample differences
    differences = correct1 - correct2
    
    if np.std(differences) == 0:
        # No variation in differences
        t_stat = 0
        p_value = 1.0
    else:
        t_stat, p_value = stats.ttest_1samp(differences, 0)
    
    # Calculate confidence interval for difference
    n = len(differences)
    se_diff = np.std(differences, ddof=1) / np.sqrt(n)
    ci_lower = acc_diff - 1.96 * se_diff
    ci_upper = acc_diff + 1.96 * se_diff
    
    return {
        'accuracy1': acc1,
        'accuracy2': acc2,
        'accuracy_diff': acc_diff,
        't_stat': t_stat,
        'p_value': p_value,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'se_diff': se_diff,
        'n_samples': n
    }

def match_samples_by_id(data1, data2, prob_key='pred_probs', label_key='labels', id_key='sample_ids'):
    """
    Match and reorder samples between two datasets based on sample IDs.
    
    Parameters:
    data1: dict-like, first dataset (e.g., loaded .npz file)
    data2: dict-like, second dataset
    prob_key: str, key for predicted probabilities
    label_key: str, key for true labels
    id_key: str, key for sample IDs
    
    Returns:
    tuple: (matched_data1, matched_data2, common_ids) where matched data contains only common samples in same order
    """
    
    # Get sample IDs
    ids1 = data1[id_key]
    ids2 = data2[id_key]
    
    # Convert to strings if needed for consistent comparison
    if isinstance(ids1[0], bytes):
        ids1 = [id_.decode('utf-8') for id_ in ids1]
    if isinstance(ids2[0], bytes):
        ids2 = [id_.decode('utf-8') for id_ in ids2]
    
    ids1 = np.array(ids1)
    ids2 = np.array(ids2)
    
    # Find common sample IDs
    common_ids = np.intersect1d(ids1, ids2)
    print(f"Total samples in model 1: {len(ids1)}")
    print(f"Total samples in model 2: {len(ids2)}")
    print(f"Common samples found: {len(common_ids)}")
    
    if len(common_ids) == 0:
        raise ValueError("No common sample IDs found between the two datasets!")
    
    # Create masks for common samples
    mask1 = np.isin(ids1, common_ids)
    mask2 = np.isin(ids2, common_ids)
    
    # Extract common samples
    common_ids1 = ids1[mask1]
    common_ids2 = ids2[mask2]
    
    # Create sorting indices to match order
    # Sort both by the common IDs to ensure same order
    sort_idx1 = np.argsort(common_ids1)
    sort_idx2 = np.argsort(common_ids2)
    
    # Get indices in original arrays
    original_indices1 = np.where(mask1)[0][sort_idx1]
    original_indices2 = np.where(mask2)[0][sort_idx2]
    
    # Extract and reorder data
    matched_data1 = {
        prob_key: data1[prob_key][original_indices1],
        label_key: data1[label_key][original_indices1],
        id_key: ids1[original_indices1]
    }
    
    matched_data2 = {
        prob_key: data2[prob_key][original_indices2],
        label_key: data2[label_key][original_indices2],
        id_key: ids2[original_indices2]
    }
    
    # Verify matching worked
    assert np.array_equal(matched_data1[id_key], matched_data2[id_key]), "Sample ID matching failed!"
    
    return matched_data1, matched_data2, common_ids

def compare_accuracy(file1_path, file2_path, model1_name="Model 1", model2_name="Model 2", 
                    prob_key='pred_probs', label_key='labels', id_key='sample_ids',
                    save_plots=True, results_dir="Results/"):
    """
    Compare accuracy between two models with proper sample ID matching.
    
    Parameters:
    file1_path: str, path to first model results (.npz file)
    file2_path: str, path to second model results (.npz file)
    model1_name: str, name for first model
    model2_name: str, name for second model
    prob_key: str, key for predicted probabilities in npz files
    label_key: str, key for true labels in npz files
    id_key: str, key for sample IDs in npz files
    save_plots: bool, whether to save plots
    results_dir: str, directory to save results
    
    Returns:
    dict: Comprehensive accuracy comparison results
    """
    
    print("Loading data...")
    # Load data
    data1 = np.load(file1_path)
    data2 = np.load(file2_path)
    
    print("Matching samples by ID...")
    # Match samples by ID
    matched_data1, matched_data2, common_ids = match_samples_by_id(
        data1, data2, prob_key, label_key, id_key
    )
    
    # Extract matched data
    y_true1 = matched_data1[label_key]
    y_true2 = matched_data2[label_key]
    
    # Verify labels are identical (they should be for same test set)
    if not np.array_equal(y_true1, y_true2):
        print("Warning: True labels differ between datasets. Using labels from first model.")
    y_true = y_true1
    
    # Get predictions
    if matched_data1[prob_key].ndim == 2:
        y_pred1 = np.argmax(matched_data1[prob_key], axis=1)
    else:
        y_pred1 = (matched_data1[prob_key] > 0.5).astype(int)
        
    if matched_data2[prob_key].ndim == 2:
        y_pred2 = np.argmax(matched_data2[prob_key], axis=1)
    else:
        y_pred2 = (matched_data2[prob_key] > 0.5).astype(int)
    
    print("Performing statistical tests...")
    # Perform statistical tests
    accuracy_results = paired_accuracy_test(y_true, y_pred1, y_pred2)
    mcnemar_results = mcnemar_test(y_true, y_pred1, y_pred2)
    
    # Calculate confusion matrices
    cm1 = confusion_matrix(y_true, y_pred1)
    cm2 = confusion_matrix(y_true, y_pred2)
    
    # Create visualization
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    
    # Accuracy Comparison Bar Plot
    accuracies = [accuracy_results['accuracy1'], accuracy_results['accuracy2']]
    colors = ['skyblue', 'lightcoral']
    bars = ax1.bar([model1_name, model2_name], accuracies, color=colors, alpha=0.8, edgecolor='black')
    ax1.set_ylabel('Accuracy', fontsize=12)
    ax1.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')
    ax1.set_ylim([0, 1])
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Add accuracy values on bars
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{acc:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # Add confidence interval
    ci_text = f"Difference: {accuracy_results['accuracy_diff']:.4f}\n"
    ci_text += f"95% CI: [{accuracy_results['ci_lower']:.4f}, {accuracy_results['ci_upper']:.4f}]"
    ax1.text(0.5, 0.1, ci_text, transform=ax1.transAxes, ha='center', 
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8), fontsize=9)
    
    # McNemar's Test Visualization
    contingency = mcnemar_results['contingency_table']
    labels = ['Both Correct', 'Model 1 Better', 'Model 2 Better', 'Both Wrong']
    values = [contingency[0,0], contingency[0,1], contingency[1,0], contingency[1,1]]
    colors_pie = ['lightgreen', 'skyblue', 'lightcoral', 'lightgray']
    
    ax2.pie(values, labels=labels, colors=colors_pie, autopct='%1.1f%%', startangle=90)
    ax2.set_title("McNemar's Test Breakdown", fontsize=14, fontweight='bold')
    
    # Confusion Matrix for Model 1
    im1 = ax3.imshow(cm1, interpolation='nearest', cmap='Blues')
    ax3.set_title(f'{model1_name} Confusion Matrix', fontsize=12, fontweight='bold')
    
    # Add colorbar
    cbar1 = plt.colorbar(im1, ax=ax3, fraction=0.046, pad=0.04)
    
    tick_marks = np.arange(len(np.unique(y_true)))
    ax3.set_xticks(tick_marks)
    ax3.set_yticks(tick_marks)
    ax3.set_xlabel('Predicted Label')
    ax3.set_ylabel('True Label')
    
    # Add text annotations
    for i in range(cm1.shape[0]):
        for j in range(cm1.shape[1]):
            ax3.text(j, i, format(cm1[i, j], 'd'),
                    ha="center", va="center", 
                    color="white" if cm1[i, j] > cm1.max() / 2 else "black",
                    fontweight='bold')
    
    # Confusion Matrix for Model 2
    im2 = ax4.imshow(cm2, interpolation='nearest', cmap='Reds')
    ax4.set_title(f'{model2_name} Confusion Matrix', fontsize=12, fontweight='bold')
    
    # Add colorbar
    cbar2 = plt.colorbar(im2, ax=ax4, fraction=0.046, pad=0.04)
    
    ax4.set_xticks(tick_marks)
    ax4.set_yticks(tick_marks)
    ax4.set_xlabel('Predicted Label')
    ax4.set_ylabel('True Label')
    
    # Add text annotations
    for i in range(cm2.shape[0]):
        for j in range(cm2.shape[1]):
            ax4.text(j, i, format(cm2[i, j], 'd'),
                    ha="center", va="center", 
                    color="white" if cm2[i, j] > cm2.max() / 2 else "black",
                    fontweight='bold')
    
    plt.tight_layout()
    
    if save_plots:
        plot_path = f"{results_dir}/accuracy_comparison.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"Accuracy comparison plot saved to: {plot_path}")
    
    plt.show()
    
    # Print detailed results
    print("\n" + "="*80)
    print("ACCURACY COMPARISON RESULTS")
    print("="*80)
    
    print(f"\n  SAMPLE INFORMATION:")
    print(f"  Total matched samples: {accuracy_results['n_samples']}")
    print(f"  Common sample IDs: {len(common_ids)}")
    
    print(f"\n  ACCURACY RESULTS:")
    print(f"  {model1_name}: {accuracy_results['accuracy1']:.4f} ({accuracy_results['accuracy1']*100:.2f}%)")
    print(f"  {model2_name}: {accuracy_results['accuracy2']:.4f} ({accuracy_results['accuracy2']*100:.2f}%)")
    print(f"  Difference: {accuracy_results['accuracy_diff']:.4f}")
    print(f"  95% Confidence Interval: [{accuracy_results['ci_lower']:.4f}, {accuracy_results['ci_upper']:.4f}]")
    
    print(f"\n  STATISTICAL TESTS:")
    print(f"  Paired t-test:")
    print(f"    T-statistic: {accuracy_results['t_stat']:.4f}")
    print(f"    P-value: {accuracy_results['p_value']:.6f}")
    t_significant = "significant" if accuracy_results['p_value'] < 0.05 else "not significant"
    print(f"    Result: Accuracy difference is {t_significant} (α = 0.05)")
    
    print(f"\n  McNemar's test:")
    print(f"    Contingency table:")
    print(f"      Both correct: {mcnemar_results['both_correct']}")
    print(f"      {model1_name} better: {mcnemar_results['model1_better']}")
    print(f"      {model2_name} better: {mcnemar_results['model2_better']}")
    print(f"      Both wrong: {mcnemar_results['both_wrong']}")
    print(f"    Discordant pairs: {mcnemar_results['discordant_pairs']}")
    if not np.isnan(mcnemar_results['chi2_stat']):
        print(f"    Chi-square: {mcnemar_results['chi2_stat']:.4f}")
    print(f"    P-value: {mcnemar_results['p_value']:.6f}")
    mcnemar_significant = "significant" if mcnemar_results['p_value'] < 0.05 else "not significant"
    print(f"    Result: Performance difference is {mcnemar_significant} (α = 0.05)")
    
    print(f"\n  SUMMARY:")
    if accuracy_results['accuracy_diff'] > 0:
        better_model = model1_name
        worse_model = model2_name
    else:
        better_model = model2_name
        worse_model = model1_name
    
    print(f"  Better performing model: {better_model}")
    print(f"  Absolute accuracy difference: {abs(accuracy_results['accuracy_diff']):.4f}")
    
    if accuracy_results['p_value'] < 0.05 or mcnemar_results['p_value'] < 0.05:
        print(f"  Statistical significance: YES")
        print(f"  Conclusion: {better_model} performs significantly better than {worse_model}")
    else:
        print(f"  Statistical significance: NO")
        print(f"  Conclusion: No significant difference in performance detected")
    
    return {
        'accuracy_results': accuracy_results,
        'mcnemar_results': mcnemar_results,
        'confusion_matrix1': cm1,
        'confusion_matrix2': cm2,
        'matched_samples': len(common_ids),
        'common_ids': common_ids
    }

# Example usage
if __name__ == "__main__":
    # Compare accuracy between two models
    results = compare_accuracy(
        file1_path="Results/FCNN_encoder_confounder_free_plots/test_results.npz",
        file2_path="Results/FCNN_plots/test_results.npz",
        model1_name="FCNN Encoder Confounder Free",
        model2_name="FCNN",
        save_plots=True,
        results_dir="Results/"
    )